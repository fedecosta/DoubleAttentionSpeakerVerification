{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/usuaris/veu/federico.costa/git_repositories/DoubleAttentionSpeakerVerification/scripts/')\n",
    "\n",
    "import argparse\n",
    "import librosa\n",
    "import librosa.display as ld\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "from model_evaluator_2 import ModelEvaluator\n",
    "from settings import MODEL_EVALUATOR_DEFAULT_SETTINGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Feature extractor class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22-10-06 22:19:24 - model_evaluator_2 - INFO - Setting device...\n",
      "22-10-06 22:19:24 - model_evaluator_2 - INFO - Running on cuda device.\n",
      "22-10-06 22:19:24 - model_evaluator_2 - INFO - Device setted.\n",
      "22-10-06 22:19:24 - model_evaluator_2 - INFO - Setting random seed...\n",
      "22-10-06 22:19:24 - model_evaluator_2 - INFO - Random seed setted.\n",
      "22-10-06 22:19:24 - model_evaluator_2 - INFO - Loading checkpoint from /home/usuaris/veu/federico.costa/git_repositories/DoubleAttentionSpeakerVerification/models/v1_tiny/v1_tiny_VGGNL_DoubleMHA_700.chkpt\n",
      "22-10-06 22:19:25 - model_evaluator_2 - INFO - Model checkpoint was saved at epoch 0\n",
      "22-10-06 22:19:25 - model_evaluator_2 - INFO - Checkpoint loaded.\n"
     ]
    }
   ],
   "source": [
    "default_params_dict = MODEL_EVALUATOR_DEFAULT_SETTINGS\n",
    "\n",
    "default_params_dict['model_checkpoint_path'] = '/home/usuaris/veu/federico.costa/git_repositories/DoubleAttentionSpeakerVerification/models/v1_tiny/v1_tiny_VGGNL_DoubleMHA_700.chkpt'\n",
    "default_params_dict['test_clients'] = '/home/usuaris/veu/federico.costa/git_repositories/DoubleAttentionSpeakerVerification/labels/test/tiny_check/clients.ndx'\n",
    "default_params_dict['test_impostors'] = '/home/usuaris/veu/federico.costa/git_repositories/DoubleAttentionSpeakerVerification/labels/test/tiny_check/impostors.ndx'\n",
    "default_params_dict['dump_folder'] = './models_results/v1___'\n",
    "default_params_dict['data_dir'] = ['/home/usuaris/veussd/DATABASES/VoxCeleb/VoxCeleb1/test/']\n",
    "\n",
    "default_params = argparse.Namespace(**default_params_dict)\n",
    "\n",
    "model_evaluator_2 = ModelEvaluator(default_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22-10-06 22:19:26 - model_evaluator_2 - INFO - Loading data from /home/usuaris/veu/federico.costa/git_repositories/DoubleAttentionSpeakerVerification/labels/test/tiny_check/clients.ndx\n",
      "22-10-06 22:19:26 - model_evaluator_2 - INFO - Loading data from /home/usuaris/veu/federico.costa/git_repositories/DoubleAttentionSpeakerVerification/labels/test/tiny_check/impostors.ndx\n",
      "/home/usuaris/veu/federico.costa/.conda/envs/DASV/lib/python3.10/site-packages/torch/utils/data/dataloader.py:563: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "22-10-06 22:19:26 - model_evaluator_2 - INFO - Data and labels loaded.\n"
     ]
    }
   ],
   "source": [
    "model_evaluator_2.load_data()\n",
    "model_evaluator_2.total_batches = len(model_evaluator_2.training_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 of 9\n",
      "torch.Size([4, 482, 80])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "\n",
    "    # Switch torch to evaluation mode\n",
    "    model_evaluator_2.net.eval()\n",
    "\n",
    "    for model_evaluator_2.batch_number, (input_1, len_1, input_2, len_2, label) in enumerate(model_evaluator_2.training_generator):\n",
    "\n",
    "        print(f\"Batch {model_evaluator_2.batch_number} of {model_evaluator_2.total_batches}\")\n",
    "\n",
    "        input_1 = input_1.float().to(model_evaluator_2.device)\n",
    "        print(input_1.size())\n",
    "        input_2 = input_2.float().to(model_evaluator_2.device)\n",
    "        len_1 = len_1.int().to(model_evaluator_2.device)\n",
    "        len_2 = len_2.int().to(model_evaluator_2.device)\n",
    "        label = label.int().to(model_evaluator_2.device)\n",
    "        \n",
    "        input_1 = [tensor[:len_1[i],:].unsqueeze(0) for i, tensor in enumerate(input_1)]\n",
    "        input_2 = [tensor[:len_2[i],:].unsqueeze(0) for i, tensor in enumerate(input_2)]\n",
    "        \n",
    "        if torch.cuda.device_count() > 1:\n",
    "            embedding_1 = [model_evaluator_2.net.module.get_embedding(input) for input in input_1]\n",
    "            embedding_2 = [model_evaluator_2.net.module.get_embedding(input) for input in input_2]\n",
    "        else:\n",
    "            embedding_1 = [model_evaluator_2.net.get_embedding(input) for input in input_1]\n",
    "            embedding_2 = [model_evaluator_2.net.get_embedding(input) for input in input_2]\n",
    "\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [93]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43membedding_1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "embedding_1.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import scoreCosineDistance\n",
    "dist = [scoreCosineDistance(emb_1, emb_2) for emb_1, emb_2 in zip(embedding_1, embedding_2)][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7450, 0.5279, 0.5160, 0.3977])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.74496925, 1), (0.5279028, 1), (0.5159511, 1), (0.39771855, 1)]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(torch.tensor(dist).cpu().detach().numpy(), label.cpu().detach().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([482, 80])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_1[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-4.9273e-02, -2.5617e-01, -2.2962e-01, -6.8943e-01, -5.4523e-01,\n",
       "         -5.4669e-01, -6.1277e-01, -4.5274e-01,  8.0051e-01, -5.8577e-01,\n",
       "          9.8880e-01, -7.6463e-01, -5.8092e-01, -1.6867e-01, -1.6307e-01,\n",
       "         -3.6484e-01, -8.2553e-01, -6.9187e-01,  1.0534e+00, -5.6010e-01,\n",
       "         -7.5977e-01,  1.6055e+00, -3.0820e-01, -6.6317e-01, -5.2201e-02,\n",
       "         -5.3053e-01, -6.8584e-01,  4.0441e-01, -7.2524e-01, -8.2029e-01,\n",
       "         -6.7179e-01, -7.2336e-01, -6.3542e-01,  7.0891e-01, -2.9244e-01,\n",
       "         -6.0293e-01, -8.1141e-01, -3.2447e-01, -5.9938e-01, -8.2280e-01,\n",
       "          5.0942e-01,  9.3585e-01,  1.4759e-01, -5.7271e-01, -5.8872e-01,\n",
       "         -7.5093e-01, -7.6130e-01, -6.9583e-01, -7.6415e-01,  4.1675e-02,\n",
       "         -6.5178e-01,  9.7255e-02, -5.9963e-01,  1.2134e+00,  7.4425e-01,\n",
       "         -3.1699e-01, -7.6056e-01,  9.1079e-03, -5.6457e-01,  7.4538e-01,\n",
       "         -8.3574e-01, -4.8584e-01,  9.1115e-01, -6.0348e-01,  2.7966e-01,\n",
       "         -7.0498e-01,  1.3194e+00,  2.8634e-01, -7.7929e-01, -5.7206e-01,\n",
       "         -7.6736e-01,  3.0088e-01, -8.0880e-01,  4.9080e-01, -6.2995e-01,\n",
       "          1.2004e+00, -7.6110e-01, -7.6246e-01, -5.8888e-01, -6.2105e-01,\n",
       "          2.8704e-02,  4.4158e-01,  1.0173e-01, -6.2478e-01, -6.4916e-01,\n",
       "         -7.8664e-01, -3.8167e-01, -6.1782e-01, -5.3308e-01, -7.8938e-01,\n",
       "         -9.5589e-02, -8.1785e-01, -8.0441e-01, -7.6848e-01,  8.6747e-01,\n",
       "         -5.4860e-01, -7.8159e-01, -7.2378e-01,  8.1855e-01,  4.0510e-01,\n",
       "          9.0443e-01,  1.7374e+00, -6.1684e-01,  8.4066e-02,  1.0138e+00,\n",
       "         -2.5584e-03, -6.0873e-01, -6.4974e-01, -7.8791e-01,  2.1471e-01,\n",
       "         -6.6179e-01, -7.7073e-01, -6.2242e-01,  8.6420e-02, -6.2626e-03,\n",
       "         -8.0928e-01, -7.5444e-01, -7.6845e-01,  2.3378e+00, -6.2743e-01,\n",
       "          5.8220e-01, -5.6991e-01,  8.9831e-01, -5.5140e-01,  2.2063e-03,\n",
       "         -2.4679e-01, -5.6677e-01,  4.5714e-01,  1.0660e+00,  5.8327e-01,\n",
       "          1.3302e+00,  5.0659e-01,  6.8413e-01, -7.9320e-01, -5.6465e-01,\n",
       "         -1.1531e-01, -4.5857e-01, -5.0701e-01,  2.2492e+00, -7.2992e-01,\n",
       "          1.2439e+00, -5.5179e-01, -5.8336e-01, -7.6659e-01, -7.6952e-01,\n",
       "         -5.1729e-01, -7.3232e-01, -6.2385e-01, -7.6825e-01, -8.1818e-01,\n",
       "          1.0848e+00,  9.2859e-01, -6.4356e-01, -1.2604e-01, -7.5793e-01,\n",
       "         -8.3915e-01, -5.9693e-01, -8.1369e-01,  8.9138e-01,  4.2965e-01,\n",
       "          5.5724e-01,  5.3073e-01, -5.6417e-01, -5.6114e-01, -7.6316e-01,\n",
       "         -6.8049e-01, -7.7334e-01, -3.9158e-01,  7.7399e-01, -2.5304e-01,\n",
       "          2.1068e+00, -6.5534e-01, -7.3580e-01, -7.8844e-01, -2.3714e-01,\n",
       "         -3.8547e-01,  2.0321e-01, -6.5755e-01, -7.1922e-01, -7.4231e-01,\n",
       "         -6.3524e-01,  1.4603e-01, -4.2641e-01, -6.6582e-01, -4.8217e-01,\n",
       "         -6.2321e-01, -8.1663e-01, -6.0871e-01,  7.1427e-01, -7.3450e-01,\n",
       "         -8.1699e-01, -7.6332e-01, -6.4531e-01,  2.7274e-01, -6.0177e-01,\n",
       "         -7.7257e-01, -6.2104e-01, -4.9171e-01, -8.0369e-01, -6.0686e-01,\n",
       "         -7.5205e-01,  2.1960e-01,  8.5087e-02, -7.3905e-01, -1.0885e-01,\n",
       "          8.6225e-01,  5.7516e-01, -8.0673e-01, -6.7696e-01, -7.4352e-01,\n",
       "         -8.1831e-02,  3.4260e-01, -6.0037e-01, -6.1037e-01, -5.6843e-01,\n",
       "         -8.2142e-01,  4.7242e-01, -3.6368e-01, -6.3575e-01,  1.7627e-01,\n",
       "          3.4703e-01, -7.7020e-01, -8.7713e-01,  9.8479e-02,  1.2672e-02,\n",
       "         -6.3467e-02,  1.5800e-01, -6.0060e-01, -8.0176e-01, -7.7960e-01,\n",
       "         -5.8775e-01, -7.9166e-01, -5.1255e-02,  2.6572e-01,  1.9051e-01,\n",
       "         -5.3115e-01,  5.2892e-02, -4.7477e-01, -7.9285e-01, -7.5643e-01,\n",
       "          3.3403e-01, -4.2644e-01, -8.7358e-01, -7.3073e-01,  1.7953e+00,\n",
       "          1.2976e+00,  1.1201e+00, -5.7408e-01, -8.3139e-01,  3.4492e-01,\n",
       "         -6.9325e-01, -1.7424e-01, -7.1502e-01, -5.8438e-01, -6.1784e-01,\n",
       "         -7.5151e-01, -2.0425e-01, -1.1112e-01, -8.3053e-01,  1.0692e+00,\n",
       "          3.3060e-01, -7.6858e-01, -1.2715e-01,  7.1902e-01, -7.8075e-01,\n",
       "         -6.2773e-01,  1.6754e-01, -7.2170e-01,  2.8338e-01, -7.3092e-01,\n",
       "         -5.9789e-01, -4.2378e-01, -3.1861e-01,  6.6338e-01, -6.0176e-01,\n",
       "         -6.3836e-01, -6.7000e-01, -4.3657e-01,  1.2299e+00, -8.3276e-01,\n",
       "         -3.2314e-01, -6.5209e-01, -5.9370e-01, -1.8829e-01,  9.4673e-01,\n",
       "         -6.2986e-01, -7.7861e-02, -8.2367e-01,  3.7288e-02, -8.2031e-01,\n",
       "          9.2127e-01, -7.8183e-01,  5.0416e-01, -7.3884e-01, -7.4489e-01,\n",
       "          1.0729e+00, -6.4760e-01, -7.8887e-01, -6.6314e-01, -5.5189e-01,\n",
       "          2.3279e-01,  1.6699e+00, -2.4157e-01, -6.6450e-01, -6.2675e-01,\n",
       "          6.7958e-01, -8.2554e-01, -6.9800e-01,  3.1019e-01,  1.1999e+00,\n",
       "         -2.3887e-01, -7.5984e-01, -7.9900e-01, -6.8757e-01, -3.5221e-01,\n",
       "          6.2675e-01, -6.7024e-01,  3.7154e-01,  4.3762e-01, -5.9219e-01,\n",
       "         -6.1355e-01, -8.5171e-01,  2.1448e+00, -7.9735e-01,  5.4628e-01,\n",
       "         -7.2181e-01,  6.4520e-02, -5.2825e-01, -5.8941e-01, -8.7531e-01,\n",
       "         -4.9786e-01, -7.7877e-01, -1.0985e-02, -7.2319e-01, -8.5391e-01,\n",
       "          3.2340e-01, -5.5269e-01, -8.4859e-01, -8.0632e-01, -6.5894e-01,\n",
       "         -5.8766e-01, -6.2816e-01, -7.0297e-01,  2.5391e-01, -7.4207e-01,\n",
       "         -9.2934e-01, -6.0522e-01,  3.6635e-02,  1.1015e+00, -8.0158e-01,\n",
       "         -8.0586e-02, -6.4354e-01, -6.6817e-01, -7.9335e-01, -8.2795e-01,\n",
       "         -7.6619e-01, -2.7668e-01, -5.7720e-01,  6.8151e-01,  5.6268e-01,\n",
       "          4.6047e-01,  1.0789e+00, -3.5188e-01, -1.9607e-02, -6.4562e-01,\n",
       "         -6.8395e-01, -6.0314e-01, -5.9525e-01, -4.9256e-01, -6.6083e-03,\n",
       "          6.8315e-02,  2.1057e-01, -8.0894e-01, -6.7566e-01, -5.1250e-02,\n",
       "          9.3046e-01,  9.2679e-02, -6.8337e-01, -3.1715e-02, -5.5608e-01,\n",
       "          1.9156e+00,  1.1723e-01, -3.4734e-01, -8.3387e-01, -7.8593e-01,\n",
       "         -7.7618e-01, -8.3394e-01, -3.3360e-01, -4.7718e-01,  1.5919e-01,\n",
       "         -8.1416e-01, -5.1947e-01,  8.6336e-02, -5.3176e-02, -2.5836e-01,\n",
       "         -5.8775e-01, -7.1272e-01, -7.2342e-01, -2.1680e-01, -7.0626e-01]],\n",
       "       device='cuda:0', grad_fn=<NativeBatchNormBackward0>)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_evaluator_2.net.get_embedding(input_1[0].unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2352, 80])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat(l).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "only one element tensors can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [77]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43ml\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "torch.tensor(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [76]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43ml\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/DASV/lib/python3.10/site-packages/torch/_tensor.py:757\u001b[0m, in \u001b[0;36mTensor.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    755\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m__array__, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m    756\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 757\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    758\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    759\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    }
   ],
   "source": [
    "np.array(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [71]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m torch\u001b[38;5;241m.\u001b[39mas_tensor(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43ml\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/.conda/envs/DASV/lib/python3.10/site-packages/torch/_tensor.py:757\u001b[0m, in \u001b[0;36mTensor.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    755\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m__array__, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m    756\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 757\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    758\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    759\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    }
   ],
   "source": [
    "torch.as_tensor(np.array(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only integer tensors of a single element can be converted to an index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [19]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43minput_2\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43mlen_2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39msize()\n",
      "\u001b[0;31mTypeError\u001b[0m: only integer tensors of a single element can be converted to an index"
     ]
    }
   ],
   "source": [
    "input_2[:,:len_2,:].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False,  ..., False, False, False],\n",
       "        [False, False, False,  ..., False, False, False],\n",
       "        [False, False, False,  ..., False, False, False],\n",
       "        ...,\n",
       "        [False, False, False,  ..., False, False, False],\n",
       "        [False, False, False,  ..., False, False, False],\n",
       "        [False, False, False,  ..., False, False, False]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_2[0][:,:] >= len_2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([498, 850, 542, 462])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2],\n",
       "        [3],\n",
       "        [5],\n",
       "        [1]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_lens = torch.tensor([2,3,5,1]).unsqueeze(-1)\n",
    "seq_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2, 3, 4]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_lens = torch.tensor([2,3,5,1]).unsqueeze(-1)\n",
    "max_len = torch.max(seq_lens)\n",
    "\n",
    "# create tensor of suitable shape and same number of dimensions\n",
    "range_tensor = torch.arange(max_len).unsqueeze(0)\n",
    "range_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2, 3, 4],\n",
       "        [0, 1, 2, 3, 4],\n",
       "        [0, 1, 2, 3, 4],\n",
       "        [0, 1, 2, 3, 4]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_lens = torch.tensor([2,3,5,1]).unsqueeze(-1)\n",
    "max_len = torch.max(seq_lens)\n",
    "\n",
    "# create tensor of suitable shape and same number of dimensions\n",
    "range_tensor = torch.arange(max_len).unsqueeze(0)\n",
    "range_tensor = range_tensor.expand(seq_lens.size(0), range_tensor.size(1))\n",
    "range_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_lens = torch.tensor([2,3,5,1]).unsqueeze(-1)\n",
    "max_len = torch.max(seq_lens)\n",
    "\n",
    "# create tensor of suitable shape and same number of dimensions\n",
    "range_tensor = torch.arange(max_len).unsqueeze(0)\n",
    "range_tensor = range_tensor.expand(seq_lens.size(0), range_tensor.size(1))\n",
    "\n",
    "# until this step, we only created auxiliary tensors (you may already have from previous steps) \n",
    "# the real mask tensor is created with binary masking:\n",
    "mask_tensor = (range_tensor < seq_lens) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2, 3, 4],\n",
       "        [0, 1, 2, 3, 4],\n",
       "        [0, 1, 2, 3, 4],\n",
       "        [0, 1, 2, 3, 4]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "range_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2],\n",
       "        [3],\n",
       "        [5],\n",
       "        [1]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True,  True, False, False, False],\n",
       "        [ True,  True,  True, False, False],\n",
       "        [ True,  True,  True,  True,  True],\n",
       "        [ True, False, False, False, False]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True,  True,  True,  True, False],\n",
       "        [False, False, False, False, False]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(10).view(2,5) < 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([498, 80])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_2[0][:len_2[0], :].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_2.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import normalizeFeatures, featureReader, TestDataset\n",
    "\n",
    "with open(model_evaluator_2.input_params.test_clients, 'r') as clients_file:\n",
    "    clients_trials = clients_file.readlines()\n",
    "\n",
    "# Read the paths of the impostors audios\n",
    "with open(model_evaluator_2.input_params.test_impostors, 'r') as impostors_file:\n",
    "    impostors_trials = impostors_file.readlines()\n",
    "\n",
    "# Instanciate a Dataset class\n",
    "dataset = TestDataset(clients_trials, impostors_trials, model_evaluator_2.params, model_evaluator_2.input_params, model_evaluator_2.net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_1.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[1,2,3],[1,2,4]])\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.from_numpy(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_1.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label[0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_example_1 = torch.cat([input_1[0], torch.ones(14, 80).to(model_evaluator_2.device)])\n",
    "tensor_example_2 = torch.cat([input_1[1], torch.ones(23, 80).to(model_evaluator_2.device)])\n",
    "tensor_example_3 = torch.cat([input_1[2], torch.ones(11, 80).to(model_evaluator_2.device)])\n",
    "\n",
    "data = [\n",
    "    (tensor_example_1, label[0].item(), tensor_example_1.size(0)), \n",
    "    (tensor_example_2, label[1].item(), tensor_example_2.size(0)),\n",
    "    (tensor_example_3, label[2].item(), tensor_example_3.size(0)),\n",
    "]\n",
    "\n",
    "features, labels, lengths = zip(*data)\n",
    "\n",
    "max_len = max(lengths)\n",
    "# We are assuming that all features have the same number of columns\n",
    "number_columns = data[0][0].size(1)\n",
    "padded_features = torch.zeros((len(data), max_len, number_columns))\n",
    "\n",
    "for i in range(len(data)):\n",
    "    features_tensor = data[i][0]\n",
    "    rows, cols = features_tensor.size(0), features_tensor.size(1)\n",
    "    padded_features_tensor = torch.cat(\n",
    "        [\n",
    "            features_tensor, \n",
    "            torch.zeros((max_len - rows, cols)).to(model_evaluator_2.device)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    padded_features[i] = padded_features_tensor\n",
    "    \n",
    "labels = torch.tensor(labels)\n",
    "lengths = torch.tensor(lengths)\n",
    "    \n",
    "padded_features.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.cat([input_1[0], torch.ones(14, 80).to(model_evaluator_2.device)])\n",
    "a.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [torch.tensor([1,2,3]), torch.tensor([3,4])]\n",
    "b = torch.nn.utils.rnn.pad_sequence(a, batch_first=True)\n",
    "b\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.utils.rnn.pack_padded_sequence(b, batch_first=True, lengths=[3,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DASV",
   "language": "python",
   "name": "dasv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
