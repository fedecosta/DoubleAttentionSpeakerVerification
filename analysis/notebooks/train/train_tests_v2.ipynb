{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/usuaris/veu/federico.costa/git_repositories/DoubleAttentionSpeakerVerification/scripts/')\n",
    "import os\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from train import Trainer\n",
    "from settings import TRAIN_DEFAULT_SETTINGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Load the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_params_dict = TRAIN_DEFAULT_SETTINGS\n",
    "\n",
    "default_params_dict['train_labels_path'] = '/home/usuaris/veu/federico.costa/git_repositories/DoubleAttentionSpeakerVerification/labels/train/tiny_check/train_labels.ndx'\n",
    "default_params_dict['valid_clients'] = '/home/usuaris/veu/federico.costa/git_repositories/DoubleAttentionSpeakerVerification/labels/valid/voxceleb_2/clients.ndx'\n",
    "default_params_dict['valid_impostors'] = '/home/usuaris/veu/federico.costa/git_repositories/DoubleAttentionSpeakerVerification/labels/valid/voxceleb_2/impostors.ndx'\n",
    "default_params_dict['eval_and_save_best_model_every'] = 10\n",
    "default_params_dict['early_stopping'] = 25\n",
    "default_params_dict['update_optimizer_every'] = 20\n",
    "default_params_dict['pooling_method'] = 'Attention'\n",
    "\n",
    "default_params = argparse.Namespace(**default_params_dict)\n",
    "\n",
    "trainer = Trainer(default_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trainer.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - Training plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.debug_info[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = [item[\"step\"] for item in trainer.debug_info]\n",
    "train_loss = [item[\"train_loss\"] for item in trainer.debug_info]\n",
    "train_eval_metric = [item[\"train_eval_metric\"] for item in trainer.debug_info]\n",
    "valid_eval_metric = [item[\"valid_eval_metric\"] for item in trainer.debug_info]\n",
    "best_train_loss = [item[\"best_train_loss\"] for item in trainer.debug_info]\n",
    "best_model_train_eval_metric = [item[\"best_model_train_eval_metric\"] for item in trainer.debug_info]\n",
    "best_model_valid_eval_metric = [item[\"best_model_valid_eval_metric\"] for item in trainer.debug_info]\n",
    "validations_without_improvement = [item[\"validations_without_improvement\"] for item in trainer.debug_info]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 10), dpi = 300)\n",
    "\n",
    "plt.subplot(4, 1, 1)\n",
    "plt.plot(steps, train_loss, linewidth = 1, label = \"train_loss\")\n",
    "# plt.plot(steps, best_train_loss, \"--\", label = \"best_train_loss\")\n",
    "plt.ylim(0, max(train_loss))\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training loss\")\n",
    "\n",
    "plt.subplot(4, 1, 2)\n",
    "plt.plot(steps, train_eval_metric, label = \"train accuracy\")\n",
    "plt.plot(steps, best_model_train_eval_metric, \"--\", label = \"best_model_train_eval_metric\")\n",
    "plt.ylim(0, 1)\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Training evaluation\")\n",
    "\n",
    "plt.subplot(4, 1, 3)\n",
    "plt.plot(steps, valid_eval_metric, label = \"validation EER\")\n",
    "plt.plot(steps, best_model_valid_eval_metric, \"--\", label = \"best_model_valid_eval_metric\")\n",
    "plt.ylim(0, 50)\n",
    "plt.ylabel(\"EER\")\n",
    "plt.title(\"Validation evaluation\")\n",
    "\n",
    "plt.subplot(4, 1, 4)\n",
    "plt.plot(steps, validations_without_improvement, label = \"validations_without_improvement\")\n",
    "plt.axhline(trainer.params.early_stopping, linestyle = \"--\", color = \"grey\")\n",
    "plt.ylim(0, trainer.params.early_stopping + 1)\n",
    "plt.ylabel(\"validations_without_improvement\")\n",
    "plt.title(\"Early stoping\")\n",
    "\n",
    "plt.xlabel(\"Step\")\n",
    "\n",
    "plt.suptitle(\"Training Process\", fontweight = 'bold', fontsize = 16)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.checkpoint.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.checkpoint['epoch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.checkpoint['model_results']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 6), dpi = 300)\n",
    "\n",
    "plt.plot(steps, train_loss, label = \"train_loss\")\n",
    "plt.plot(steps, best_train_loss, \"--\", label = \"best_train_loss\")\n",
    "\n",
    "plt.ylim(0, max(train_loss))\n",
    "plt.xlabel(\"Step\")\n",
    "\n",
    "plt.title(\"Training Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 6), dpi = 300)\n",
    "\n",
    "plt.plot(steps, train_eval_metric, label = \"train accuracy\")\n",
    "plt.plot(steps, best_model_train_eval_metric, \"--\", label = \"best_model_train_eval_metric\")\n",
    "\n",
    "plt.ylim(0, 1)\n",
    "plt.xlabel(\"Step\")\n",
    "\n",
    "plt.title(\"Evaluation metrics\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 6), dpi = 300)\n",
    "\n",
    "plt.plot(steps, valid_eval_metric, label = \"valid EER\")\n",
    "plt.plot(steps, best_model_valid_eval_metric, \"--\", label = \"best_model_valid_eval_metric\")\n",
    "\n",
    "plt.ylim(0, 50)\n",
    "plt.xlabel(\"Step\")\n",
    "\n",
    "plt.title(\"Evaluation metrics\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 - Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_number, (input, label) in enumerate(trainer.training_generator):\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(trainer.params.train_labels_path, 'r') as data_labels_file:\n",
    "    train_labels = data_labels_file.readlines()\n",
    "    \n",
    "train_label = train_labels[0]\n",
    "\n",
    "pickle_path = train_label.replace(\"\\n\", \"\").split(\" \")[0] + \".pickle\"\n",
    "\n",
    "with open(pickle_path, 'rb') as pickle_file:\n",
    "    features_dict = pickle.load(pickle_file)\n",
    "    \n",
    "features = features_dict[\"features\"]\n",
    "features_settings = features_dict[\"settings\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DASV",
   "language": "python",
   "name": "dasv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
