{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/usuaris/veu/federico.costa/git_repositories/DoubleAttentionSpeakerVerification/scripts/')\n",
    "import os\n",
    "import librosa\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import torch\n",
    "from model import SpeakerClassifier\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_labels(paths):\n",
    "    \n",
    "    labels = []\n",
    "    for path in paths:\n",
    "        with open(path, 'r') as data_labels_file:\n",
    "            labels = labels + data_labels_file.readlines()\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_audio_path(audio_path, prepend_directories):\n",
    "\n",
    "    # we need to remove the first \"/\" to join paths\n",
    "    if audio_path[0] == \"/\":\n",
    "        audio_path = audio_path[1:]\n",
    "\n",
    "    # remove the file extension, if has\n",
    "    if len(audio_path.split(\".\")) > 1:\n",
    "        audio_path = '.'.join(audio_path.split(\".\")[:-1]) \n",
    "\n",
    "    # We prepend prepend_directory to the paths and the file extension\n",
    "    data_founded = False\n",
    "    for dir in prepend_directories:\n",
    "\n",
    "        if data_founded == False:\n",
    "\n",
    "            for audio_format in ['wav', 'm4a']:\n",
    "\n",
    "                audio_file = f\"{audio_path}.{audio_format}\"\n",
    "                complete_audio_file_path = os.path.join(dir, audio_file)\n",
    "\n",
    "                if os.path.exists(complete_audio_file_path):\n",
    "                    data_founded = True\n",
    "                    break\n",
    "\n",
    "    assert data_founded, f\"{audio_path} not founded.\"\n",
    "\n",
    "    return complete_audio_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_audio_paths(labels_list, prepend_directories):\n",
    "\n",
    "    speaker_1_audio_paths = []\n",
    "    speaker_2_audio_paths = []\n",
    "    total_lines = len(labels_list)\n",
    "    progress_pctg_to_print = 0\n",
    "    for index, label in enumerate(labels_list):\n",
    "\n",
    "        label = label.replace('\\n', '')\n",
    "        label_chunks = label.split(' ')\n",
    "\n",
    "        if len(label_chunks) == 2:\n",
    "\n",
    "            # Validation or Test labels\n",
    "            # label is of the form '/speaker/interview/file /speaker/interview/file'\n",
    "            audio_path_1, audio_path_2 = label_chunks\n",
    "            audio_path_1 = format_audio_path(audio_path_1, prepend_directories)\n",
    "            audio_path_2 = format_audio_path(audio_path_2, prepend_directories)\n",
    "            speaker_1_audio_paths.append(audio_path_1)\n",
    "            speaker_2_audio_paths.append(audio_path_2)\n",
    "\n",
    "        else:\n",
    "\n",
    "            assert False, f\"{label} has a not expected structure.\"\n",
    "\n",
    "        progress_pctg = index / total_lines * 100\n",
    "        if progress_pctg >=  progress_pctg_to_print:\n",
    "            #print(f\"{progress_pctg:.0f}% paths processed...\")\n",
    "            progress_pctg_to_print = progress_pctg_to_print + 1\n",
    "            \n",
    "    return speaker_1_audio_paths, speaker_2_audio_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepend_directories = [\n",
    "    '/home/usuaris/veussd/DATABASES/VoxCeleb/VoxCeleb2/dev/',\n",
    "]\n",
    "\n",
    "clients_labels_file_name = [\n",
    "    '/home/usuaris/veu/federico.costa/git_repositories/DoubleAttentionSpeakerVerification/labels/valid/voxceleb_2/22_12_09_12_41_06_12nc5wq4_fiery-donkey-13/clients.ndx',\n",
    "]\n",
    "impostors_labels_file_name = [\n",
    "    '/home/usuaris/veu/federico.costa/git_repositories/DoubleAttentionSpeakerVerification/labels/valid/voxceleb_2/22_12_09_12_41_29_ikfavyhj_ruby-microwave-14/impostors.ndx',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "clients_labels_list = load_labels(clients_labels_file_name)\n",
    "\n",
    "speaker_1_audio_paths, speaker_2_audio_paths = get_audio_paths(clients_labels_list[:10], prepend_directories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"speaker_1_audio_path\" : speaker_1_audio_paths[:1000],\n",
    "        \"speaker_2_audio_path\" : speaker_2_audio_paths[:1000],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_speaker(audio_path):\n",
    "    \n",
    "    speaker_chunk = [chunk for chunk in audio_path.split(\"/\") if chunk.startswith(\"id\")]\n",
    "    # Only consider directories with /id.../\n",
    "    if len(speaker_chunk) > 0: \n",
    "        speaker = speaker_chunk[0]\n",
    "    \n",
    "    return speaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_duration(audio_path):\n",
    "\n",
    "    audio_duration = librosa.get_duration(filename = audio_path)\n",
    "    \n",
    "    return audio_duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_extension(path, new_extension):\n",
    "    \n",
    "    path = '.'.join(path.split(\".\")[:-1])\n",
    "    path = f\"{path}.{new_extension}\"\n",
    "    \n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"speaker_1\"] = df[\"speaker_1_audio_path\"].apply(lambda x: get_speaker(x))\n",
    "df[\"speaker_2\"] = df[\"speaker_2_audio_path\"].apply(lambda x: get_speaker(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"speaker_1_feature_path\"] = df[\"speaker_1_audio_path\"].apply(lambda x: replace_extension(x, \"pickle\"))\n",
    "df[\"speaker_2_feature_path\"] = df[\"speaker_2_audio_path\"].apply(lambda x: replace_extension(x, \"pickle\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 141 ms, sys: 54.8 ms, total: 196 ms\n",
      "Wall time: 1.46 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df[\"duration_seconds_audio_speaker_1\"] = df[\"speaker_1_audio_path\"].apply(lambda x: get_duration(x))\n",
    "\n",
    "df[\"duration_seconds_audio_speaker_2\"] = df[\"speaker_2_audio_path\"].apply(lambda x: get_duration(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speaker_1_audio_path</th>\n",
       "      <th>speaker_2_audio_path</th>\n",
       "      <th>speaker_1</th>\n",
       "      <th>speaker_2</th>\n",
       "      <th>duration_seconds_audio_speaker_1</th>\n",
       "      <th>duration_seconds_audio_speaker_2</th>\n",
       "      <th>speaker_1_feature_path</th>\n",
       "      <th>speaker_2_feature_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/usuaris/veussd/DATABASES/VoxCeleb/VoxCel...</td>\n",
       "      <td>/home/usuaris/veussd/DATABASES/VoxCeleb/VoxCel...</td>\n",
       "      <td>id08821</td>\n",
       "      <td>id08821</td>\n",
       "      <td>8.3</td>\n",
       "      <td>4.5</td>\n",
       "      <td>/home/usuaris/veussd/DATABASES/VoxCeleb/VoxCel...</td>\n",
       "      <td>/home/usuaris/veussd/DATABASES/VoxCeleb/VoxCel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/usuaris/veussd/DATABASES/VoxCeleb/VoxCel...</td>\n",
       "      <td>/home/usuaris/veussd/DATABASES/VoxCeleb/VoxCel...</td>\n",
       "      <td>id04274</td>\n",
       "      <td>id04274</td>\n",
       "      <td>12.3</td>\n",
       "      <td>11.2</td>\n",
       "      <td>/home/usuaris/veussd/DATABASES/VoxCeleb/VoxCel...</td>\n",
       "      <td>/home/usuaris/veussd/DATABASES/VoxCeleb/VoxCel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/usuaris/veussd/DATABASES/VoxCeleb/VoxCel...</td>\n",
       "      <td>/home/usuaris/veussd/DATABASES/VoxCeleb/VoxCel...</td>\n",
       "      <td>id08637</td>\n",
       "      <td>id08637</td>\n",
       "      <td>7.2</td>\n",
       "      <td>5.1</td>\n",
       "      <td>/home/usuaris/veussd/DATABASES/VoxCeleb/VoxCel...</td>\n",
       "      <td>/home/usuaris/veussd/DATABASES/VoxCeleb/VoxCel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/home/usuaris/veussd/DATABASES/VoxCeleb/VoxCel...</td>\n",
       "      <td>/home/usuaris/veussd/DATABASES/VoxCeleb/VoxCel...</td>\n",
       "      <td>id08860</td>\n",
       "      <td>id08860</td>\n",
       "      <td>4.0</td>\n",
       "      <td>11.7</td>\n",
       "      <td>/home/usuaris/veussd/DATABASES/VoxCeleb/VoxCel...</td>\n",
       "      <td>/home/usuaris/veussd/DATABASES/VoxCeleb/VoxCel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/home/usuaris/veussd/DATABASES/VoxCeleb/VoxCel...</td>\n",
       "      <td>/home/usuaris/veussd/DATABASES/VoxCeleb/VoxCel...</td>\n",
       "      <td>id01470</td>\n",
       "      <td>id01470</td>\n",
       "      <td>5.5</td>\n",
       "      <td>4.2</td>\n",
       "      <td>/home/usuaris/veussd/DATABASES/VoxCeleb/VoxCel...</td>\n",
       "      <td>/home/usuaris/veussd/DATABASES/VoxCeleb/VoxCel...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                speaker_1_audio_path  \\\n",
       "0  /home/usuaris/veussd/DATABASES/VoxCeleb/VoxCel...   \n",
       "1  /home/usuaris/veussd/DATABASES/VoxCeleb/VoxCel...   \n",
       "2  /home/usuaris/veussd/DATABASES/VoxCeleb/VoxCel...   \n",
       "3  /home/usuaris/veussd/DATABASES/VoxCeleb/VoxCel...   \n",
       "4  /home/usuaris/veussd/DATABASES/VoxCeleb/VoxCel...   \n",
       "\n",
       "                                speaker_2_audio_path speaker_1 speaker_2  \\\n",
       "0  /home/usuaris/veussd/DATABASES/VoxCeleb/VoxCel...   id08821   id08821   \n",
       "1  /home/usuaris/veussd/DATABASES/VoxCeleb/VoxCel...   id04274   id04274   \n",
       "2  /home/usuaris/veussd/DATABASES/VoxCeleb/VoxCel...   id08637   id08637   \n",
       "3  /home/usuaris/veussd/DATABASES/VoxCeleb/VoxCel...   id08860   id08860   \n",
       "4  /home/usuaris/veussd/DATABASES/VoxCeleb/VoxCel...   id01470   id01470   \n",
       "\n",
       "   duration_seconds_audio_speaker_1  duration_seconds_audio_speaker_2  \\\n",
       "0                               8.3                               4.5   \n",
       "1                              12.3                              11.2   \n",
       "2                               7.2                               5.1   \n",
       "3                               4.0                              11.7   \n",
       "4                               5.5                               4.2   \n",
       "\n",
       "                              speaker_1_feature_path  \\\n",
       "0  /home/usuaris/veussd/DATABASES/VoxCeleb/VoxCel...   \n",
       "1  /home/usuaris/veussd/DATABASES/VoxCeleb/VoxCel...   \n",
       "2  /home/usuaris/veussd/DATABASES/VoxCeleb/VoxCel...   \n",
       "3  /home/usuaris/veussd/DATABASES/VoxCeleb/VoxCel...   \n",
       "4  /home/usuaris/veussd/DATABASES/VoxCeleb/VoxCel...   \n",
       "\n",
       "                              speaker_2_feature_path  \n",
       "0  /home/usuaris/veussd/DATABASES/VoxCeleb/VoxCel...  \n",
       "1  /home/usuaris/veussd/DATABASES/VoxCeleb/VoxCel...  \n",
       "2  /home/usuaris/veussd/DATABASES/VoxCeleb/VoxCel...  \n",
       "3  /home/usuaris/veussd/DATABASES/VoxCeleb/VoxCel...  \n",
       "4  /home/usuaris/veussd/DATABASES/VoxCeleb/VoxCel...  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = '/home/usuaris/veu/federico.costa/git_repositories/DoubleAttentionSpeakerVerification/models/22_12_14_09_48_59_vgg_tmh_ap_fc_VGGNL_TransformerStackedAttentionPooling_c0whzsyq/22_12_14_09_48_59_vgg_tmh_ap_fc_VGGNL_TransformerStackedAttentionPooling_c0whzsyq.chkpt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "checkpoint = torch.load(checkpoint_path, map_location = device)\n",
    "params = checkpoint['settings']\n",
    "\n",
    "net = SpeakerClassifier(params, device)\n",
    "\n",
    "try:\n",
    "    net.load_state_dict(checkpoint['model'])\n",
    "except RuntimeError:    \n",
    "    net.module.load_state_dict(checkpoint['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/usuaris/veussd/DATABASES/VoxCeleb/VoxCeleb2/dev/id08821/A1zaOlZpvdQ/00074.pickle'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"speaker_1_feature_path\"].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/usuaris/veussd/DATABASES/VoxCeleb/VoxCeleb2/dev/id08821/D0nlv29V-9c/00104.pickle'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"speaker_2_feature_path\"].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(features, normalization = 'cmn'):\n",
    "\n",
    "    # Cepstral mean normalization\n",
    "    if normalization == 'cmn':\n",
    "\n",
    "        # Compute the mean for each frequency band (columns)\n",
    "        mean = np.mean(features, axis = 0)\n",
    "\n",
    "        # Substract for each column the corresponding column mean\n",
    "        features = features - mean\n",
    "\n",
    "    # Cepstral mean and variance normalization\n",
    "    elif normalization == 'cmvn':\n",
    "\n",
    "        # Compute the mean for each frequency band (columns)\n",
    "        mean = np.mean(features, axis = 0)\n",
    "\n",
    "        # Substract for each column the corresponding column mean\n",
    "        features = features - mean\n",
    "\n",
    "        # Compute the standard deviation for each frequency band (columns)\n",
    "        std = np.std(features, axis = 0)\n",
    "\n",
    "        # HACK guess this is to avoid zero division overflow\n",
    "        std = np.where(std > 0.01, std, 1.0)\n",
    "\n",
    "        # Divide for each column the corresponding column std\n",
    "        features = features / std\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_vector(utterance_path):\n",
    "\n",
    "    # Load the spectrogram saved in pickle format\n",
    "    with open(utterance_path, 'rb') as pickle_file:\n",
    "        features_dict = pickle.load(pickle_file)\n",
    "\n",
    "    features = features_dict[\"features\"]\n",
    "    features = np.transpose(features)\n",
    "    features = normalize(features)\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [52]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     embedding_2 \u001b[38;5;241m=\u001b[39m net\u001b[38;5;241m.\u001b[39mmodule\u001b[38;5;241m.\u001b[39mget_embedding(input_2)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 15\u001b[0m     embedding_1 \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     embedding_2 \u001b[38;5;241m=\u001b[39m net\u001b[38;5;241m.\u001b[39mget_embedding(input_2)\n",
      "File \u001b[0;32m~/git_repositories/DoubleAttentionSpeakerVerification/scripts/model.py:152\u001b[0m, in \u001b[0;36mSpeakerClassifier.get_embedding\u001b[0;34m(self, input_tensor)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_embedding\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_tensor):\n\u001b[1;32m    149\u001b[0m \n\u001b[1;32m    150\u001b[0m     \u001b[38;5;66;03m# TODO should we use relu and bn in every layer?d\u001b[39;00m\n\u001b[0;32m--> 152\u001b[0m     encoder_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfront_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;66;03m# TODO seems that alignment is not used anywhere\u001b[39;00m\n\u001b[1;32m    155\u001b[0m     embedding_0, alignment \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpoolingLayer(encoder_output)\n",
      "File \u001b[0;32m~/.conda/envs/DASV/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/git_repositories/DoubleAttentionSpeakerVerification/scripts/front_end.py:108\u001b[0m, in \u001b[0;36mVGGNL.forward\u001b[0;34m(self, input_tensor)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_tensor):\n\u001b[1;32m     99\u001b[0m \n\u001b[1;32m    100\u001b[0m     \u001b[38;5;66;03m# input_tensor dimensions are:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;66;03m# We need to add a new dimension corresponding to the channels\u001b[39;00m\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;66;03m# This channel dimension will be 1 because the spectrogram has only 1 channel\u001b[39;00m\n\u001b[1;32m    107\u001b[0m     input_tensor \u001b[38;5;241m=\u001b[39m  input_tensor\u001b[38;5;241m.\u001b[39mview( \n\u001b[0;32m--> 108\u001b[0m         \u001b[43minput_tensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m,  \n\u001b[1;32m    109\u001b[0m         input_tensor\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m), \n\u001b[1;32m    110\u001b[0m         \u001b[38;5;241m1\u001b[39m, \n\u001b[1;32m    111\u001b[0m         input_tensor\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m2\u001b[39m),\n\u001b[1;32m    112\u001b[0m         )\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;66;03m# We need to put the channel dimension first because nn.Conv2d need it that way\u001b[39;00m\n\u001b[1;32m    115\u001b[0m     input_tensor \u001b[38;5;241m=\u001b[39m input_tensor\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not callable"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "\n",
    "    # Switch torch to evaluation mode\n",
    "    net.eval()\n",
    "    \n",
    "    input_1, input_2 = [\n",
    "        get_feature_vector(df[\"speaker_1_feature_path\"].iloc[0]), \n",
    "        get_feature_vector(df[\"speaker_1_feature_path\"].iloc[1]),\n",
    "    ]\n",
    "    \n",
    "    if torch.cuda.device_count() > 1:\n",
    "        embedding_1 = net.module.get_embedding(input_1)\n",
    "        embedding_2 = net.module.get_embedding(input_2)\n",
    "    else:\n",
    "        embedding_1 = net.get_embedding(input_1)\n",
    "        embedding_2 = net.get_embedding(input_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DASV",
   "language": "python",
   "name": "dasv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
